Nyvac096Microbiome

Jacob A. Cram
jcram@fhcrc.org
cramjaco@gmail.com

This directory accompanies the (in prep as of this writing) manuscript "The human gut microbiota associated with baselin eimmune status and response to HIV vaccines". It contains materials to run both the upstream processing of the microbiome data (demultiplexing with Qiime, sequence variant assignment with DADA2, phylogenetic tree with phangorn, SV taxonomic assignment with DADA2) and downstream analysis (statistics in a jupyter notebook file in R).

The downstream analysis can be run without re-doing the upstream portion. We default to using the files generated by the upstream analysis from our initial run, for consistancy between runs. There appears to be some variability in the results that one gets between upstream runs.


# Dependencies:

## R version 3.4.1. 
Notes on R
I have not had success with all of the subsequent dependencies when using `condas` to install R.
https://unix.stackexchange.com/questions/149451/install-r-in-my-own-directory
When I tested these scripts I built R 3.4.1 from source on a clean virtual box containing Ubuntu 18.04. 

The following packages were required (I installed with apt) for my R build.
`build-essential fort77 xorg-dev liblzma-dev  libblas-dev gfortran gcc-multilib gobjc++ libreadline-dev libbz2-dev libcurl4-openssl-dev texlive-fonts-extra texinfo default-jdk`
And of course, add R to path.

## Anacondas
Must contain jupyter notebook.

## For upstream analysis.
 * To run the demultiplexing, you will need to install qiime1. I recommend using anacondas to set up the following environment
 
` conda create -n qiime1 numpy=1.10 python=2.7 qiime matplotlib=1.4.3 mock nose -c bioconda`


     

## For downstream analyis:
###jupyter notebook or jupyter lab running in conda
 
### I have been using the r package `packrat` to keep track of packages.
Some dependencies that were required on my system 
`libssl-dev libxml2-dev`
`t1-xfree86-nonfree ttf-xfree86-nonfree ttf-xfree86-nonfree-syriac xfonts-75dpi xfonts-100dpi`
`libcairo2-dev`

To bring packages back, I would advise installing packrat from within R.
This seems to run automatically if one runs R from inside the main directory.
If not, try:
`install.packages('packrat')`
and then restoring from snapshot
`packrat::restore()`
 
*Note* - I had been trying to use `condas` to install R packages, but didn not have success 
Activate *irkernel* from within R to connect it to jupyter notebook.
*Jupyter notebook must be installed and then the system restarted before this command will work*

```
IRkernel::installspec()
```


# How to run analyses
## Upstream analysis

Upstream analysis is not necessary to redo the downstream analysis.

The order for this analysis is:
 * demultiplex
 * call SVs
 * make tree
 * generate taxonomic information.

These scripts can be called in order by calling, from the `scripts\` directory
`all_upstream.sh`

On systems running slurm (such as Fred Hutch's rhino cluster), you can call `sbatch scripts/upstream.sbatch` in order to request a 16 node cluster. This should take about 8 hours to run. The slow step is remaking the phylogenetic tree. If I was going to do this again from scratch, I'd probably use raxml.

all_upstream.sh just calls other scripts, those pieces can be run as follows:

Individual pieces can be run as follows:

 * To demultiplex, run `sh scripts/demultBothPlates.sh`
 * The next three scripts must be run inside of the nyvac-lab-2 environment
 `source attach nyvac-lab-2`
 * To remake dada2 sequence varients run `Rscript dada2work-March2018Run.R`. One can also open the r script and run it in any R interpreter. (This is true of all of the subsequent R steps. Such a process makes for substantially easier debugging.
 * To make the phylogenetic tree `Rscript makeTree.R`
 * To generate taxonomic information first acquire necessary training data by running `sh pull_training.sh`. Then run `Rscript dada2taxonomy-March2018Run.R`.

## Downstream analysis.

This can be run independently of the downstream analysis. It defaults to using data from the `data\` directory. In theory, all one should need to do is open the Mar2018_096.ipynb file in jupyter notebook or jupyter lab and run all of the cells.

If you want to run it on re-analyzed data, find comment out the line  `#upOriginal <- TRUE` and uncomment the line `upOriginal <- FALSE`. 

If you want the script to run faster, set `jnperm <- 9999`, the cost of this is that the p-values are not calculated as precisely. If you want p-values that don't fluctuate from run to run, set `jnperm <- 99999` and maybe go get lunch or similar while the file is running.
